{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Noise_Dataloader.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BQmYo2rqmK4"
      },
      "source": [
        "# Get Dataset from Google Drive  \n",
        "Please upload your dataset on google drive first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vxHB3wfW911",
        "outputId": "dfbc24c6-108a-4224-ffce-b801d231986a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbDhYfKKa41s"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "import tqdm\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "file_name = \"Multimedia_dataset.zip\"\n",
        "zip_path = os.path.join('/content/drive/MyDrive/lab/Multimedia_dataset.zip')\n",
        "\n",
        "!cp \"{zip_path}\" .\n",
        "!unzip -q \"{file_name}\"\n",
        "!rm \"{file_name}\"\n",
        "\n",
        "\n",
        "file_list = os.listdir('/content/train')\n",
        "for file in tqdm(file_list):\n",
        "  image = Image.open(\"/content/train/\" + str(file))\n",
        "  inverted_image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "  inverted_image.save('/content/train/' + 'inverted_' + str(file) + '.png')\n",
        "  rotated_image = image.rotate(90)\n",
        "  rotated_image.save('/content/train/' + 'rotated90_' + str(file) + '.png')\n",
        "  rotated_image = image.rotate(180)\n",
        "  rotated_image.save('/content/train/' + 'rotated180_' + str(file) + '.png')\n",
        "  rotated_image = image.rotate(270)\n",
        "  rotated_image.save('/content/train/' + 'rotated270_' + str(file) + '.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmbp65jgq67j"
      },
      "source": [
        "# Noise Transform  \n",
        "If you want to change how much noise you are giving, change the stddev and mean values at 'gaussian_noise' function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyeUBdtKYQSu"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms\n",
        "\n",
        "import random\n",
        "\n",
        "class NoiseTransform(object):\n",
        "  def __init__(self, size=180, mode=\"training\"):\n",
        "    super(NoiseTransform, self).__init__()\n",
        "    self.size = size\n",
        "    self.mode = mode\n",
        "  \n",
        "  def gaussian_noise(self, img):\n",
        "    mean = 0\n",
        "    stddev = 25\n",
        "    noise = Variable(torch.zeros(img.size()))\n",
        "    noise = noise.data.normal_(mean, stddev/255.)\n",
        "\n",
        "    return noise\n",
        "\n",
        "  def __call__(self, img):\n",
        "    if (self.mode == \"training\") | (self.mode == \"validation\"):\n",
        "      self.gt_transform = transforms.Compose([\n",
        "        # transforms.RandomCrop(self.size),\n",
        "        transforms.Resize((self.size, self.size), interpolation=2),\n",
        "        transforms.ToTensor()])\n",
        "      self.noise_transform = transforms.Compose([\n",
        "        # transforms.RandomCrop(self.size),\n",
        "        transforms.Resize((self.size, self.size), interpolation=2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(self.gaussian_noise),\n",
        "      ])\n",
        "      return self.gt_transform(img), self.noise_transform(img)\n",
        "\n",
        "    elif self.mode == \"testing\":\n",
        "      self.gt_transform = transforms.Compose([\n",
        "        # transforms.Resize((self.size, self.size), interpolation=2),\n",
        "        transforms.ToTensor()])\n",
        "      return self.gt_transform(img)\n",
        "    else:\n",
        "      return NotImplementedError\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGKbE7uFrWwb"
      },
      "source": [
        "# Dataloader for Noise Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxZsXXZ9YpYp"
      },
      "source": [
        "import torch\n",
        "import torch.utils.data  as data\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "class NoiseDataset(data.Dataset):\n",
        "  def __init__(self, root_path, size):\n",
        "    super(NoiseDataset, self).__init__()\n",
        "\n",
        "    self.root_path = root_path\n",
        "    self.size = size\n",
        "    self.transforms = None\n",
        "    self.examples = None\n",
        "\n",
        "  def set_mode(self, mode):\n",
        "    self.mode = mode\n",
        "    self.transforms = NoiseTransform(self.size, mode)\n",
        "    if mode == \"training\":\n",
        "      train_dir = os.path.join(self.root_path, \"train\")\n",
        "      self.examples = [os.path.join(self.root_path, \"train\", dirs) for dirs in os.listdir(train_dir)]\n",
        "    elif mode == \"validation\":\n",
        "      val_dir = os.path.join(self.root_path, \"validation\")\n",
        "      self.examples = [os.path.join(self.root_path, \"validation\", dirs) for dirs in os.listdir(val_dir)]\n",
        "    elif mode == \"testing\":\n",
        "      test_dir = os.path.join(self.root_path, \"test\")\n",
        "      self.examples = [os.path.join(self.root_path, \"test\", dirs) for dirs in os.listdir(test_dir)]\n",
        "    else:\n",
        "      raise NotImplementedError\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.examples)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    file_name = self.examples[idx]\n",
        "    image = Image.open(file_name)\n",
        "\n",
        "    if self.mode == \"testing\":\n",
        "      input_img = self.transforms(image)\n",
        "      sample = {\"img\": input_img}\n",
        "    else:\n",
        "      clean, noise = self.transforms(image)\n",
        "      sample = {\"img\": clean, \"noise\": noise}\n",
        "\n",
        "    return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDx4vFU-rf5z"
      },
      "source": [
        "# Example for Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53tDtBFLenTz"
      },
      "source": [
        "import torch\n",
        "import torch.utils.data  as data\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "def image_show(img):\n",
        "  if isinstance(img, torch.Tensor):\n",
        "    img = transforms.ToPILImage()(img)\n",
        "  plt.imshow(img)\n",
        "  plt.show()\n",
        "\n",
        "# Change to your data root directory\n",
        "root_path = \"/content/\"\n",
        "# Depend on runtime setting\n",
        "use_cuda = True\n",
        "\n",
        "train_dataset = NoiseDataset(root_path, 128)\n",
        "train_dataset.set_mode(\"training\")\n",
        "\n",
        "train_dataloader = data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "for i, data in enumerate(tqdm.tqdm(train_dataloader)):\n",
        "  if use_cuda:\n",
        "    img = data[\"img\"].to('cuda')\n",
        "    noise = data[\"noise\"].to('cuda')\n",
        "  \n",
        "  model_input = img + noise\n",
        "  noise_image = torch.clamp(model_input, 0, 1)\n",
        "\n",
        "\n",
        "  if i % 100 == 0:\n",
        "    image_show(img[0])\n",
        "    image_show(noise_image[0])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "840r5BR0Xpy_"
      },
      "source": [
        "# model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOEIi97L9Y3H"
      },
      "source": [
        "import torch \n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from math import log10\n",
        "from torch.autograd import Variable\n",
        "from skimage.util import random_noise\n",
        "from google.colab.patches import cv2_imshow\n",
        "from torchsummary import summary\n",
        "\n",
        "def conv3x3(in_chn, out_chn, bias=True):\n",
        "    layer = nn.Conv2d(in_chn, out_chn, kernel_size=3, stride=1, padding=1, bias=bias)\n",
        "    return layer\n",
        "\n",
        "def conv_down(in_chn, out_chn, bias=False): #픽셀 절반으로 down\n",
        "    layer = nn.Conv2d(in_chn, out_chn, kernel_size=4, stride=2, padding=1, bias=bias)\n",
        "    return layer\n",
        "\n",
        "def conv(in_channels, out_channels, kernel_size, bias=False, stride = 1):\n",
        "    return nn.Conv2d(\n",
        "        in_channels, out_channels, kernel_size,\n",
        "        padding=(kernel_size//2), bias=bias, stride = stride)\n",
        "\n",
        "## Supervised Attention Module\n",
        "class SAM(nn.Module):\n",
        "    def __init__(self, n_feat, kernel_size=3, bias=True):\n",
        "        super(SAM, self).__init__()\n",
        "        self.conv1 = conv(n_feat, n_feat, kernel_size, bias=bias)\n",
        "        self.conv2 = conv(n_feat, 3, kernel_size, bias=bias)\n",
        "        self.conv3 = conv(3, n_feat, kernel_size, bias=bias)\n",
        "\n",
        "    def forward(self, x, x_img):\n",
        "        x1 = self.conv1(x)\n",
        "        img = self.conv2(x) + x_img\n",
        "        x2 = torch.sigmoid(self.conv3(img))\n",
        "        x1 = x1*x2\n",
        "        x1 = x1+x\n",
        "        return x1, img\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.down_path_1 = nn.ModuleList()\n",
        "        self.down_path_2 = nn.ModuleList()\n",
        "        self.up_path_1 = nn.ModuleList()\n",
        "        self.up_path_2 = nn.ModuleList()\n",
        "        self.skip_conv_1 = nn.ModuleList()\n",
        "        self.skip_conv_2 = nn.ModuleList()\n",
        "\n",
        "        self.conv_01 = nn.Conv2d(3, 64, 3, 1, 1) # 픽셀 변화 없이 채널만 3 -> 64\n",
        "        self.conv_02 = nn.Conv2d(3, 64, 3, 1, 1) # 픽셀 변화 없이 채널만 3 -> 64\n",
        "\n",
        "        self.down_path_1.append(UNetDown(64, 64, True, 0.2))\n",
        "        self.down_path_1.append(UNetDown(64, 128, True, 0.2))\n",
        "        self.down_path_1.append(UNetDown(128, 256, True, 0.2))\n",
        "        self.down_path_1.append(UNetDown(256, 512, True, 0.2))\n",
        "        self.down_path_1.append(UNetDown(512, 1024, False, 0.2))\n",
        "\n",
        "        self.up_path_1.append(UNetUp(1024, 512, 0.2))\n",
        "        self.skip_conv_1.append(nn.Conv2d(512, 512, 3, 1, 1))\n",
        "\n",
        "        self.up_path_1.append(UNetUp(512, 256, 0.2))\n",
        "        self.skip_conv_1.append(nn.Conv2d(256, 256, 3, 1, 1))\n",
        "\n",
        "        self.up_path_1.append(UNetUp(256, 128, 0.2))\n",
        "        self.skip_conv_1.append(nn.Conv2d(128, 128, 3, 1, 1))\n",
        "\n",
        "        self.up_path_1.append(UNetUp(128, 64, 0.2))\n",
        "        self.skip_conv_1.append(nn.Conv2d(64, 64, 3, 1, 1))\n",
        "\n",
        "\n",
        "        self.sam12 = SAM(64)\n",
        "        self.cat12 = nn.Conv2d(128, 64, 1, 1, 0)\n",
        "\n",
        "        self.down_path_2.append(UNetDown(64, 64, True, 0.2))\n",
        "        self.down_path_2.append(UNetDown(64, 128, True, 0.2))\n",
        "        self.down_path_2.append(UNetDown(128, 256, True, 0.2))\n",
        "        self.down_path_2.append(UNetDown(256, 512, True, 0.2))\n",
        "        self.down_path_2.append(UNetDown(512, 1024, False, 0.2))\n",
        "\n",
        "        self.up_path_2.append(UNetUp(1024, 512, 0.2))\n",
        "        self.skip_conv_2.append(nn.Conv2d(512, 512, 3, 1, 1))\n",
        "\n",
        "        self.up_path_2.append(UNetUp(512, 256, 0.2))\n",
        "        self.skip_conv_2.append(nn.Conv2d(256, 256, 3, 1, 1))\n",
        "\n",
        "        self.up_path_2.append(UNetUp(256, 128, 0.2))\n",
        "        self.skip_conv_2.append(nn.Conv2d(128, 128, 3, 1, 1))\n",
        "\n",
        "        self.up_path_2.append(UNetUp(128, 64, 0.2))\n",
        "        self.skip_conv_2.append(nn.Conv2d(64, 64, 3, 1, 1))\n",
        "\n",
        "        self.last = conv3x3(64, 3, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        image = x\n",
        "\n",
        "        x1 = self.conv_01(image) #3c -> 64c\n",
        "        encs = []\n",
        "        for i, down in enumerate(self.down_path_1):\n",
        "            if (i+1) < 5:\n",
        "                x1, x1_up = down(x1)\n",
        "                encs.append(x1_up)\n",
        "            else:\n",
        "                x1 = down(x1)\n",
        "\n",
        "        for i, up in enumerate(self.up_path_1):\n",
        "            x1 = up(x1, self.skip_conv_1[i](encs[-i-1]))\n",
        "\n",
        "\n",
        "        sam_feature, out_1 = self.sam12(x1, image)\n",
        "\n",
        "        x2 = self.conv_02(image)\n",
        "        x2 = self.cat12(torch.cat([x2, sam_feature], dim=1))\n",
        "        \n",
        "        encs2 = []\n",
        "        for i, down in enumerate(self.down_path_2):\n",
        "            if (i+1) < 5:\n",
        "                x2, x2_up = down(x2)\n",
        "                encs2.append(x2_up)\n",
        "            else:\n",
        "                x2 = down(x2)\n",
        "\n",
        "        for i, up in enumerate(self.up_path_1):\n",
        "            x2 = up(x2, self.skip_conv_2[i](encs2[-i-1]))\n",
        "\n",
        "        out_2 = self.last(x2)\n",
        "        out_2 = out_2 + image\n",
        "        return [out_1, out_2]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class UNetDown(nn.Module):\n",
        "    def __init__(self, in_size, out_size, downsample, relu_slope):\n",
        "        super(UNetDown, self).__init__()\n",
        "        self.downsample = downsample\n",
        "        self.identity = nn.Conv2d(in_size, out_size, 1, 1, 0)\n",
        "\n",
        "        self.conv_1 = nn.Conv2d(in_size, out_size, kernel_size=3, padding=1, bias=True) #채널 변경, 픽셀변경 없음\n",
        "        self.relu_1 = nn.LeakyReLU(relu_slope, inplace=False)# 리키렐루 경사각만 조정\n",
        "        self.conv_2 = nn.Conv2d(out_size, out_size, kernel_size=3, padding=1, bias=True) #채널 픽셀 변경없이 conv 1층 추가\n",
        "        self.relu_2 = nn.LeakyReLU(relu_slope, inplace=False)# 리키렐루 경사각만 조정\n",
        "\n",
        "        if downsample:\n",
        "            self.downsample = conv_down(out_size, out_size, bias=False) #크기 절반으로 down\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv_1(x)\n",
        "\n",
        "        out = self.relu_1(out)\n",
        "        out = self.relu_2(self.conv_2(out))\n",
        "\n",
        "        out += self.identity(x)\n",
        "\n",
        "        if self.downsample:\n",
        "            out_down = self.downsample(out)\n",
        "            return out_down, out\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "\n",
        "class UNetUp(nn.Module):\n",
        "    def __init__(self, in_size, out_size, relu_slope):\n",
        "        super(UNetUp, self).__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2, bias=True)\n",
        "        self.conv_block = UNetConvBlock(in_size, out_size, False, relu_slope)\n",
        "\n",
        "    def forward(self, x, bridge):\n",
        "        up = self.up(x)\n",
        "        out = torch.cat([up, bridge], 1)\n",
        "        out = self.conv_block(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class skip_blocks(nn.Module):\n",
        "\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super(skip_blocks, self).__init__()\n",
        "        self.blocks = nn.ModuleList()\n",
        "        self.blocks.append(UNetConvBlock(in_size, 128, False, 0.2))\n",
        "        self.blocks.append(UNetConvBlock(128, out_size, False, 0.2))\n",
        "        self.shortcut = nn.Conv2d(in_size, out_size, kernel_size=1, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        sc = self.shortcut(x)\n",
        "        for m in self.blocks:\n",
        "            x = m(x)\n",
        "        return x + sc\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IA7ru91XwqJ"
      },
      "source": [
        "summary(Net().cuda(), (3, 128, 128)) #모델 요약"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW73AiomTKgc"
      },
      "source": [
        "def weights_init_kaiming(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n",
        "    elif classname.find('Linear') != -1:\n",
        "        nn.init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        # nn.init.uniform(m.weight.data, 1.0, 0.02)\n",
        "        m.weight.data.normal_(mean=0, std=math.sqrt(2./9./64.)).clamp_(-0.025,0.025)\n",
        "        nn.init.constant(m.bias.data, 0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFsDiqYWX3G3"
      },
      "source": [
        "import math\n",
        "\n",
        "model = Net().cuda() #모델 정의\n",
        "# model.apply(weights_init_kaiming)\n",
        "\n",
        "# model = torch.load(\"/content/model_epoch_35.pth\")\n",
        "# model = model['arch'].cuda()\n",
        "# model.load_state_dict(model['state_dict'])  # state_dict를 불러 온 후, 모델에 저장\n",
        "\n",
        "\n",
        "MSE = nn.MSELoss()\n",
        "# MSE = PSNRLoss()\n",
        "# fn_loss = CharbonnierLoss()\n",
        "\n",
        "# model = torch.load(\"/content/model_epoch_5.pth\")\n",
        "# model = model['arch'].cuda()\n",
        "\n",
        "# fn_loss = CharbonnierLoss()\n",
        "# optimizer = optim.Adam(model.parameters(),lr=1e-4)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-6, betas=(0.9, 0.999),eps=1e-8, weight_decay=1e-8)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=lr) # Adam Optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZoqWXC-e8EO"
      },
      "source": [
        "def remove_noise(model, image): # 노이즈 이미지 - model output 이미지 계산 함수\n",
        "    out = torch.clamp(image - model(image), 0., 1.)\n",
        "    # out = image - model(image)\n",
        "    out = out.cpu().clone()\n",
        "    out = out.squeeze(0)\n",
        "    trans = transforms.ToPILImage()\n",
        "    plt.imshow(trans(out))\n",
        "    plt.show()\n",
        "\n",
        "def img_show(image): # 노이즈 이미지 - model output 이미지 계산 함수\n",
        "    out = torch.clamp(image, 0., 1.)\n",
        "    # out = image\n",
        "    out = out.cpu().clone()\n",
        "    out = out.squeeze(0)\n",
        "    trans = transforms.ToPILImage()\n",
        "    plt.imshow(trans(out))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q59zZ4X_X-4L"
      },
      "source": [
        "from skimage.measure.simple_metrics import compare_psnr\n",
        "def batch_PSNR(img, imclean, data_range):\n",
        "    Img = img.data.cpu().numpy().astype(np.float32)\n",
        "    Iclean = imclean.data.cpu().numpy().astype(np.float32)\n",
        "    PSNR = 0\n",
        "    for i in range(Img.shape[0]):\n",
        "        PSNR += compare_psnr(Iclean[i, :, :, :], Img[i, :, :, :], data_range=data_range)\n",
        "    return (PSNR/Img.shape[0])\n",
        "\n",
        "ts = transforms.ToPILImage() # tensor 배열을 이미지로 변환\n",
        "def train(epoch): # epoch 만큼 학습 반복\n",
        "    epoch_loss = 0\n",
        "    for iteration, data in enumerate(train_dataloader, 1):\n",
        "\n",
        "\n",
        "        model.train()\n",
        "        model.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        target = data[\"img\"] #target = 원본 사진\n",
        "        noise = data[\"noise\"] #노이즈만 있는 사진\n",
        "        model_input = target + noise\n",
        "\n",
        "\n",
        "        noise = Variable(noise.cuda())\n",
        "        target = Variable(target.cuda())\n",
        "        model_input = Variable(model_input.cuda())\n",
        "\n",
        "        output = model(model_input)\n",
        "        \n",
        "        psnr_train = batch_PSNR(output[1],target, 1.)\n",
        "        psnr_train2 = batch_PSNR(output[0],target, 1.)\n",
        "        # loss = np.sum([fn_loss(torch.clamp(output[j],0,1),target) for j in range(len(output))])\n",
        "        loss = MSE(output[1], target) + MSE(output[0], target)\n",
        "        epoch_loss += loss.item() \n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        if iteration % 100 == 0:\n",
        "\n",
        "          # remove_noise(model,model_input[0].view([-1, 3, 128, 128]).cuda())\n",
        "          # img_show(real_output[0].view([-1, 3, 128, 128]).cuda())\n",
        "          # img_show(output[0][0].view([-1, 3, 128, 128]).cuda())\n",
        "          # img_show(output[1][0].view([-1, 3, 128, 128]).cuda())\n",
        "          # img_show(output[2][0].view([-1, 3, 128, 128]).cuda())\n",
        "          img_show(output[0][0].view([-1, 3, 128, 128]).cuda())\n",
        "          img_show(output[1][0].view([-1, 3, 128, 128]).cuda())\n",
        "          img_show(target[0].view([-1, 3, 128, 128]).cuda())\n",
        "          img_show(model_input[0].view([-1, 3, 128, 128]).cuda())\n",
        "\n",
        "          # img_show(output[0].view([-1, 3, 128, 128]).cuda())\n",
        "\n",
        "        print(\"Epoch[{}]({}/{}): Loss: {:.4f} : psnr {},psnr {}\".format(epoch, iteration, len(train_dataloader), loss.item(), psnr_train,psnr_train2))\n",
        "\n",
        "    print(\"Epoch {} Complete: Avg. Loss: {:.4f}\".format(epoch, epoch_loss / len(train_dataloader)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4oTa2-yYwVG"
      },
      "source": [
        "def save_checkpoint(state): #epoch 마다 모델 저장\n",
        "    model_out_path = \"model_epoch_{}.pth\".format(epoch)\n",
        "    torch.save(state, model_out_path)\n",
        "    print(\"Checkpoint saved to {}\".format(model_out_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBRFf5KyYxi9"
      },
      "source": [
        "num_epochs = 1000\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train(epoch)\n",
        "    if epoch % 1 == 0:\n",
        "      save_checkpoint({\n",
        "          'epoch': epoch + 1,\n",
        "          'arch': model,\n",
        "          'state_dict': model.state_dict(),\n",
        "          'optimizer' : optimizer.state_dict(),\n",
        "      })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeeHx68BY2IP"
      },
      "source": [
        "model = torch.load('/content/model_epoch_35.pth') # 모델 경로 불러오기\n",
        "model = model['arch'] # 모델 구조 추출\n",
        "\n",
        "def image_loader(image_name): # 이미지 로더\n",
        "    image = Image.open(image_name)\n",
        "    image = loader(image).float()\n",
        "    image = Variable(image, requires_grad=True)\n",
        "    image = image.unsqueeze(0) \n",
        "\n",
        "    return image.cuda()\n",
        "\n",
        "val_dataset = NoiseDataset(root_path, 128)\n",
        "val_dataset.set_mode(\"validation\")\n",
        "\n",
        "print(len(val_dataset))\n",
        "loss = 0\n",
        "for img in val_dataset:\n",
        "  model_input = img[\"img\"] + img[\"noise\"]\n",
        "  output = model(model_input.view([-1, 3, 128, 128]).cuda())\n",
        "  psnr_train = batch_PSNR(output[1],img[\"img\"].view([-1, 3, 128, 128]), 1.)\n",
        "  loss += psnr_train\n",
        "  print(psnr_train)\n",
        "\n",
        "print(loss / 500)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}